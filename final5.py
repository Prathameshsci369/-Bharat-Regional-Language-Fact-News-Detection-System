import streamlit as st
import pandas as pd
import altair as alt
import os
import time
import json # Necessary for json.dump in run_analysis
from typing import List, Dict, Any

# --- MOCK DATA DEFINITIONS (Moved from final3.py to break circular import) ---

# Mock data reflecting the structure generated by analysis_core.py, 
# including the added 'source_url' field for clean display.
MOCK_ANALYSIS_RESULTS: List[Dict[str, Any]] = [
    {
        "claim": "Harshad Mehta was the biggest scamster of Independent India.",
        "classification": "False",
        "reason": "The claim was contradicted in the Reddit post stating 'Harshad Mehta was NOT the single biggest scamster of Independent India.'",
        "source": "Local Model Analysis (Mock)",
        "batch_id": "batch_01",
        "chunk_count": 5,
        "source_url": "https://www.reddit.com/r/india/comments/abc123/scam_1992_debate_on_mehtas_legacy/" 
    },
    {
        "claim": "Harshad Mehta was trapped by bureaucrats, politicians, and media journalists.",
        "classification": "True",
        "reason": "The claim is supported by the Reddit post stating 'Harshad Mehta was trapped by bureaucrats, politicians and media journalists.'",
        "source": "Local Model Analysis (Mock)",
        "batch_id": "batch_01",
        "chunk_count": 5,
        "source_url": "https://www.reddit.com/r/indianews/comments/def456/conspiracy_behind_harshad_mehtas_downfall/"
    },
    {
        "claim": "The series 'Scam 1992' was produced by Applause Entertainment, a subsidiary of Aditya Birla Group.",
        "classification": "True",
        "reason": "The claim is supported by multiple Reddit posts stating that the series was produced by Applause Entertainment, which is a subsidiary of Aditya Birla Group.",
        "source": "Local Model Analysis (Mock)",
        "batch_id": "batch_01",
        "chunk_count": 5,
        "source_url": "https://www.reddit.com/r/bollywood/comments/ghi789/applause_entertainment_ownership_trivia/"
    },
    {
        "claim": "The Securities and Exchange Board of India (SEBI) was instrumental in exposing the scam.",
        "classification": "Misleading",
        "reason": "While SEBI played a role, the initial exposure and key investigation were led by journalist Sucheta Dalal, making the claim partially true but framed to misdirect credit.",
        "source": "Local Model Analysis (Mock)",
        "batch_id": "batch_01",
        "chunk_count": 5,
        "source_url": "https://www.reddit.com/r/askindia/comments/123xyz/role_of_sebi_in_1992_scam/"
    },
    {
        "claim": "Foreign institutional investors (FIIs) were the primary source of funds for the manipulation.",
        "classification": "False",
        "reason": "The manipulation primarily involved funds from local banks and financial institutions, not FIIs.",
        "source": "Local Model Analysis (Mock)",
        "batch_id": "batch_01",
        "chunk_count": 5,
        "source_url": "https://www.reddit.com/r/financial_history/comments/456abc/harshad_mehta_scam_funding_sources/"
    }
]

def get_mock_results():
    """Returns the mock analysis results for UI testing."""
    return MOCK_ANALYSIS_RESULTS

def get_mock_query():
    """Returns the mock query used to generate these results."""
    # Using a relevant mock query for the data above
    return "Scam 1992" 

# --- Imports for Core Logic ---
# These modules must be present in the same directory for the app to run.
try:
    from reddit import RedditScraper
    from final import run_full_analysis_pipeline
    # Removed: from final3 import get_mock_results, get_mock_query
    
    # Check if necessary files exist for live analysis
    ANALYSIS_CORE_READY = True
except ImportError as e:
    # If core files are missing, the app will run in TEST MODE only.
    # Note: The error related to final3 is now resolved by moving its contents here.
    st.error(f"Missing required module: {e}. Running in **Test Mode** only.")
    ANALYSIS_CORE_READY = False

# --- CONSTANTS ---
INPUT_FILE = "reddit_search_output.json"
MOCK_QUERY_DEFAULT = "Harshad Mehta Scam 1992" # Used as a default placeholder text

# --- UI Helper Functions ---

def get_color_hex(classification: str) -> str:
    """Returns a consistent hex color for a classification."""
    classification = classification.lower()
    if "true" in classification:
        return "#4CAF50"  # Bright Green
    elif "false" in classification:
        return "#FF5252"  # Bright Red
    elif "misleading" in classification:
        return "#FFEB3B"  # Bright Yellow
    else:
        return "#B0BEC5"  # Light Gray

def get_status_icon(classification: str) -> str:
    """Returns an emoji icon based on classification."""
    classification = classification.lower()
    if "true" in classification:
        return "‚úÖ"
    elif "false" in classification:
        return "‚ùå"
    elif "misleading" in classification:
        return "‚ö†Ô∏è"
    else:
        return "‚ùì"

def render_analysis_card(claim: Dict[str, Any]):
    """Renders a single analysis claim in a styled Streamlit card."""
    classification = claim.get('classification', 'Unverifiable')
    icon = get_status_icon(classification)
    color = get_color_hex(classification)

    st.markdown(
        f"""
        <div style="border: 2px solid {color}; padding: 15px; border-radius: 10px; margin-bottom: 15px; background-color: #1E1E1E;">
            <p style="font-size: 1.1em; font-weight: bold; color: {color}; margin: 0;">
                {icon} {classification.upper()} CLAIM
            </p>
            <p style="font-size: 1.05em; margin-top: 5px; margin-bottom: 10px;">
                <strong>Claim:</strong> {claim.get('claim', 'N/A')}
            </p>
            <p style="font-size: 0.9em; margin-bottom: 5px;">
                <strong>Reason:</strong> {claim.get('reason', 'N/A')}
            </p>
            <p style="font-size: 0.85em; color: #aaa;">
                <strong>Source URL:</strong> 
                <a href="{claim.get('source_url', '#')}" target="_blank" style="color: #4DA1FF;">
                    {claim.get('source_url', 'N/A')}
                </a>
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )

def create_visualization(results: List[Dict[str, Any]]):
    """Creates an Altair bar chart visualization of claim classifications."""
    df = pd.DataFrame(results)
    
    # Count occurrences of each classification
    classification_counts = df['classification'].str.title().value_counts().reset_index()
    classification_counts.columns = ['Classification', 'Count']
    
    # Map colors for the chart
    color_map = {
        'True': get_color_hex('True'),
        'False': get_color_hex('False'),
        'Misleading': get_color_hex('Misleading'),
        'Unverifiable': get_color_hex('Unverifiable')
    }
    
    # Define the order for the bars
    order = ['True', 'False', 'Misleading', 'Unverifiable']
    
    chart = alt.Chart(classification_counts).mark_bar().encode(
        x=alt.X('Classification:N', sort=order, axis=None),  # Hide x-axis title/labels
        y=alt.Y('Count:Q', axis=alt.Axis(title='Number of Claims', grid=True)),
        color=alt.Color('Classification:N', scale=alt.Scale(domain=list(color_map.keys()), range=list(color_map.values()))),
        tooltip=['Classification', 'Count']
    ).properties(
        title='Claim Classification Distribution'
    ).interactive() 

    # Add text labels on the bars
    text = chart.mark_text(
        align='center',
        baseline='bottom',
        dy=-5,
        color='white'
    ).encode(
        text='Count:Q',
        tooltip=alt.Tooltip('Classification:N')
    )
    
    # Combine the bar chart and the text labels
    st.altair_chart(chart + text, use_container_width=True)


# --- Main Application Logic ---

def run_analysis(query: str):
    """Runs the full analysis pipeline."""
    with st.spinner(f"Searching Reddit for '{query}' and scraping data..."):
        # Step 1: SCRAPING
        scraper = RedditScraper()
        if not scraper.reddit:
            st.error("Failed to initialize Reddit scraper. Check PRAW configuration.")
            st.session_state.analysis_complete = True
            return

        scraped_data = scraper.search_and_fetch_top_posts(query, limit=5, num_comments=5, num_subcomments=2)
        if not scraped_data:
            st.warning(f"No relevant Reddit posts found for: {query}.")
            st.session_state.analysis_complete = True
            return

        try:
            # Need to open and write the file from the scraped data
            with open(INPUT_FILE, "w", encoding='utf-8') as f:
                json.dump(scraped_data, f, indent=4, ensure_ascii=False)
            st.success(f"Scraped {len(scraped_data)} posts and saved to {INPUT_FILE}.")
        except IOError as e:
            st.error(f"Error saving scraped data: {e}")
            st.session_state.analysis_complete = True
            return

    # Step 2 & 3: CHUNKING, BATCHING, AND ANALYSIS
    with st.spinner("Analyzing claims using local LLM (Phi-4)... This may take a moment."):
        # Pass the query and the file path to the core analysis pipeline
        final_claims = run_full_analysis_pipeline(input_file=INPUT_FILE) 
        
        st.session_state.results = final_claims
        st.session_state.query = query
        st.session_state.analysis_complete = True


def main():
    """Defines the Streamlit application layout and flow."""
    st.set_page_config(layout="wide", page_title="Bharat: Regional Language Fact Verification")

    st.markdown(
        """
        <style>
        .stButton>button {
            border: 2px solid #4CAF50;
            background-color: #262730;
            color: #4CAF50;
            border-radius: 8px;
            padding: 10px 20px;
            font-size: 1.1em;
            transition: all 0.3s ease;
        }
        .stButton>button:hover {
            background-color: #4CAF50;
            color: white;
            box-shadow: 0 4px 12px rgba(76, 175, 80, 0.4);
        }
        footer {visibility: hidden;}
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # Main title and short description
    st.title("üáÆüá≥ Bharat: Regional Language Fact News Detection System")
    st.subheader("Multilingual, evidence-driven AI for real-time fact verification across 22 Indian languages")

    # --- Project Overview (expanded by default) ---
    with st.expander("üß† About This Project", expanded=True):
        st.markdown(
            """
            **Bharat** is an agentic AI system that continuously monitors social media and news sources to detect
            emerging misinformation and verify factual claims in **22 Indian languages**.

            Key capabilities:
            - Extracts and decomposes claims from platforms like **Reddit**, **YouTube**, and regional news portals.
            - Verifies atomic claims using trusted sources (PIB, ECI, Wikipedia) via entailment-based checks.
            - Produces structured outputs (ClaimReview JSON) with confidence scores, citations, and clear explanations.
            - Escalates low-confidence or politically sensitive items for **human review**.
            """
        )

    # --- Sidebar/Configuration ---
    with st.sidebar:
        # Short tagline for immediate context
        st.markdown("""
        ### üß≠ Bharat AI ‚Äî Real-Time Regional Fact Verification
        Continuously monitors and verifies factual accuracy across India's diverse language ecosystem.
        """)
        st.markdown("---")

        st.header("App Mode")
        mode = st.radio("Select Analysis Mode:", ["Live Analysis (Full Pipeline)", "Test Mode (Mock Data)"])
        
        # Initialize session state variables
        if 'results' not in st.session_state:
            st.session_state.results = []
        if 'query' not in st.session_state:
            st.session_state.query = ""
        if 'analysis_complete' not in st.session_state:
            st.session_state.analysis_complete = False

        if mode == "Test Mode (Mock Data)":
            st.session_state.results = get_mock_results()
            st.session_state.query = get_mock_query()
            st.session_state.analysis_complete = True
            st.warning("Running in **Test Mode** using mock data for demonstration.")
        
        st.markdown("---")
        st.info("Live Analysis requires `reddit.py` and `final.py` with PRAW credentials and a local Phi-4 LLM.")

    # --- Input Section ---
    query = st.text_input(
        "Enter a claim or topic to fact-check (try: 'Harshad Mehta Scam 1992'):", 
        value=st.session_state.query if st.session_state.query else MOCK_QUERY_DEFAULT,
        key="input_query"
    )
    
    # Button 1 (for when analysis core is ready)
    if st.button("Start Analysis", key="start_analysis_live") and ANALYSIS_CORE_READY and mode == "Live Analysis (Full Pipeline)":
        if query:
            # Reset state before starting a new run
            st.session_state.results = []
            st.session_state.analysis_complete = False
            run_analysis(query)
        else:
            st.warning("Please enter a query to start the analysis.")
            
    # Button 2 (for when analysis core is NOT ready, shows an error)
    elif st.button("Start Analysis", key="start_analysis_error") and not ANALYSIS_CORE_READY and mode == "Live Analysis (Full Pipeline)":
        st.error("Cannot run Live Analysis. Required core modules are missing or import failed.")


    # --- Results Display Section ---
    # Ensure these variables are defined by pulling from session state.
    results_to_display = st.session_state.results
    query_display = st.session_state.query
    
    if st.session_state.get('analysis_complete', False) and results_to_display:
        
        st.subheader(f"2. Analysis Report: {len(results_to_display)} Claims Found")
        st.write(f"Results for query: **{query_display}** ({'Live Analysis' if mode == 'Live Analysis (Full Pipeline)' else 'Mock Data'})")

        # Create columns for visualization and summary stats
        col_chart, col_metrics = st.columns([2, 1])
        
        # --- Metrics (Summary Stats) ---
        with col_metrics:
            st.markdown("### Claim Summary")
            
            # Counts correctly use results_to_display
            true_count = sum(1 for r in results_to_display if "true" in r.get('classification', '').lower())
            false_count = sum(1 for r in results_to_display if "false" in r.get('classification', '').lower())
            misleading_count = sum(1 for r in results_to_display if "misleading" in r.get('classification', '').lower())
            unverifiable_count = len(results_to_display) - (true_count + false_count + misleading_count)
            
            st.metric("‚úÖ True Claims", true_count, delta_color="normal")
            st.metric("‚ùå False Claims", false_count, delta_color="normal")
            st.metric("‚ö†Ô∏è Misleading Claims", misleading_count, delta_color="normal")
            st.metric("‚ùì Unverifiable Claims", unverifiable_count, delta_color="normal")
            
        # --- Visualization (Bar Chart) ---
        with col_chart:
            create_visualization(results_to_display)

        st.markdown("---")
        st.markdown("### Detailed Claim Analysis")
        # Display claims in styled cards
        for claim in results_to_display:
            render_analysis_card(claim)
    
    elif st.session_state.get('analysis_complete', False) and not results_to_display:
        st.info(f"Analysis completed for '{query_display}', but no claims were extracted by the LLM or no data was scraped.")

    # --- Footer (non-intrusive) ---
    st.markdown("---")
    st.markdown(
        "<div style='text-align:center; color:#888; font-size:12px;'>Built for the Agentic AI - Misinformation Track ‚Ä¢ Developed by Team Bharat</div>",
        unsafe_allow_html=True
    )



if __name__ == '__main__':
    main()

